% !TEX root = ../thesis-sample.tex

\chapter{Experimental Results}\label{chap:Experiments}

In this chapter, we present three experiments that demonstrate the efficacy of the proposed exact occupancy grid mapping and autonomous exploration from earlier chapters. The first example involves a ground vehicle mapping and exploring a 2D space. The second example is a quadrotor unmanned aerial vehicle (UAV) flying around a vertically-uniform space while generating a 3D map from level flight. The final example shows autonomous exploration completely in 3D, where a quadrotor must map and explore around complicated objects.

\section{2D Exploration With an Unmanned Ground Vehicle}
\label{sec:PioneerNRL}

The experiment involves a Pioneer ground vehicle producing a probabilistic occupancy grid map in real time. The robot motion is governed by the information gain-maximizing policy described in Chapter \ref{chap:ae2D}.


\subsection{Hardware Configuration}
The Pioneer 3 ground robot (Figure \ref{fig:Pioneer}) is chosen for this experiment because of its reliability and simple operation. The vehicle accepts two inputs, namely linear velocity (aligned with the robot wheels) and angular velocity (about a central axis passing vertically through the robot). The robot pose is estimated via Vicon Tracker, which provides the location and attitude of a rigid body. The depth readings are obtained by a Kinect depth sensor. Since the experiment is to map and explore a 2D environment, only a single central row of the 3D Kinect depth scan provides the necessary measurements. The robot captures an experimental environment with walls constructed from Styrofoam.

\begin{figure}
	\centering{
    	\begin{subfigure}[b]{0.3\textwidth}
	\centering
        		\includegraphics[height=0.7\textwidth]{pioneer.png}
        		\caption{Pioneer 3 Vehicle}
        		\label{fig:PioneerStockImage}
    	\end{subfigure}
	\begin{subfigure}[b]{0.3\textwidth}
	\centering
        		\includegraphics[height=0.7\textwidth]{pioneer_side.png}
        		\caption{Side View}
        		\label{fig:PioneerSide}
    	\end{subfigure}
	\begin{subfigure}[b]{0.3\textwidth}
	\centering
        		\includegraphics[height=0.7\textwidth]{pioneer_kinect.png}
        		\caption{Front View with Kinect}
        		\label{fig:PioneerSide}
    	\end{subfigure}
	}
\caption{Pioneer Robot}
	\medskip
	\small
	The Pioneer 3 robot serves as a reliable experimental platform for 2D mapping and autonomous exploration using a Microsoft Kinect.
\label{fig:Pioneer}
\end{figure}

\subsection{Software Configuration}

The software structure is similar to ROS nodes Section \ref{sec:Ae2DNumExamples}, which simply involves a mapping node, an exploration node, and a visualization node using OpenGL libraries~\cite{openGL}. The only differences are eliminating the simulator (Stage) and adding synchronization among the sensors, which is of high importance to the proposed occupancy grid mapping approach. Any time delay between the robot pose estimation and the sensor depth readings can cause conflicting information, which may be harmful to the probabilistic map. Thus, ROS Message Filters are applied such that the pose from Vicon Tracker ($100$ Hz) and the Microsoft Kinect ($30$ Hz) are nearly synchronized, which provides validity of the assumption that the measurement ray positions and directions are known deterministically.

\subsection{Exploring and Mapping a 2D Environment}
The environment consists of Styrofoam walls on a flat floor. The perimeter is rectangular, with several obstacles and angled walls (see Figure \ref{fig:ExpSetupPhoto}). The robot autonomously explored the space using the complete Cartesian searching approach, and Dijkstra's algorithm provides collision-free waypoints to the optimal future poses. Based on these waypoints, a constrained least squares polynomial fitting generates a smooth trajectory to follow the waypoints with fixed velocity. Finally, a simple controller is implemented such that the Pioneer follows the trajectory by moving and orienting toward the desired robot location $1$ sec in the future.

\begin{figure}
	\centering
    	\begin{subfigure}[b]{0.45\textwidth}
        		\includegraphics[width=\textwidth]{test_setup_bottom_left_cropped.jpg}
        		\caption{Bottom-Left View}
        		\label{fig:Experiment_blv}
    	\end{subfigure}
	\hspace*{0.05\columnwidth}
	\begin{subfigure}[b]{0.45\textwidth}
        		\includegraphics[width=\textwidth]{test_setup_bottom_right_cropped.jpg}
        		\caption{Bottom-Right View}
        		\label{fig:Experiment_brv}
    	\end{subfigure}
\caption{2D Experimental Environment}
	\medskip
	\small
	Images from two perspectives show the walls and obstacles of the experimental environment.
\label{fig:ExpSetupPhoto}
\end{figure}


The resulting occupancy grid maps and trajectories are depicted in Figure \ref{fig:ExperimentOGM} and a video is available at \href{https://www.youtube.com/watch?v=CRQfhhICSj0&feature=youtu.be}{\WriteBlue{www.youtube.com/watch?v=CRQfhhICSj0\&feature=youtu.be}}. Most importantly, the robot builds a clear occupancy grid map despite sensor noise and imperfect sensor synchronization. The autonomous exploration successfully guides the robot such that it builds a map of the reachable space without colliding with any obstacles.

\begin{figure}
	\centering{
    	\begin{subfigure}[b]{0.19\textwidth}
        		\includegraphics[trim={13cm 1cm 13cm 0}, clip, width=\textwidth]{feb23_t0sec.jpg}
        		\caption{$t=0$ sec}
        		\label{fig:Experiment_ogm_t0}
    	\end{subfigure}
	\begin{subfigure}[b]{0.19\textwidth}
        		\includegraphics[trim={13cm 1cm 13cm 0}, clip, width=\textwidth]{feb23_t20sec.jpg}
        		\caption{$t=20$ sec}
        		\label{fig:Experiment_ogm_t0p5}
    	\end{subfigure}    
	\begin{subfigure}[b]{0.19\textwidth}
        		\includegraphics[trim={13cm 1cm 13cm 0}, clip, width=\textwidth]{feb23_t40sec.jpg}
        		\caption{$t=40$ sec}
        		\label{fig:Experiment_ogm_t1}
    	\end{subfigure}
	\begin{subfigure}[b]{0.19\textwidth}
        		\includegraphics[trim={13cm 1cm 13cm 0}, clip, width=\textwidth]{feb23_t60sec.jpg}
        		\caption{$t=60$ sec}
        		\label{fig:Experiment_ogm_t1p5}
    	\end{subfigure}
	\begin{subfigure}[b]{0.19\textwidth}
        		\includegraphics[trim={13cm 1cm 13cm 0}, clip, width=\textwidth]{feb23_t80sec.jpg}
        		\caption{$t=80$ sec}
        		\label{fig:Experiment_ogm_t2}
    	\end{subfigure}
		\vspace*{0.05\textwidth}

	}
	\centering{
    	\begin{subfigure}[b]{0.19\textwidth}
        		\includegraphics[trim={13cm 1cm 13cm 0}, clip, width=\textwidth]{feb23_t100sec.jpg}
        		\caption{$t=100$ sec}
        		\label{fig:Experiment_ogm_t2p5}
    	\end{subfigure}
	\begin{subfigure}[b]{0.19\textwidth}
        		\includegraphics[trim={13cm 1cm 13cm 0}, clip, width=\textwidth]{feb23_t120sec.jpg}
        		\caption{$t=120$ sec}
        		\label{fig:Experiment_ogm_t3}
    	\end{subfigure}    
	\begin{subfigure}[b]{0.19\textwidth}
        		\includegraphics[trim={13cm 1cm 13cm 0}, clip, width=\textwidth]{feb23_t140sec.jpg}
        		\caption{$t=140$ sec}
        		\label{fig:Experiment_ogm_t3p5}
    	\end{subfigure}
	\begin{subfigure}[b]{0.19\textwidth}
        		\includegraphics[trim={13cm 1cm 13cm 0}, clip, width=\textwidth]{feb23_t160sec.jpg}
        		\caption{$t=160$ sec}
        		\label{fig:Experiment_ogm_t4}
    	\end{subfigure}
	\begin{subfigure}[b]{0.19\textwidth}
        		\includegraphics[trim={13cm 1cm 13cm 0}, clip, width=\textwidth]{feb23_t180sec.jpg}
        		\caption{$t=180$ sec}
        		\label{fig:Experiment_ogm_t4p5}
    	\end{subfigure}
	}
\caption{Experimental 2D Occupancy Grid Map Results}
	\medskip
	\small
	A robot autonomously explores the experimental environment and produces an occupancy grid map along the way. The robot (shaded green circle body with shaded blue circle sensor) is controlled toward the desired pose $1$ sec in the future (unshaded gray robot), which is following a constrained least-squares trajectory (cyan curve). This trajectory is based on waypoints (black circles) from Dijkstra's algorithm to arrive at the optimal pose (unshaded green circle body with unshaded blue circle sensor) as proposed in this paper.
\label{fig:ExperimentOGM}
\end{figure}

The resulting map and exploration commands are generated in real time. The grid cells are stored as double-float variables in a vector, where the vector index is mapped to a location on the occupancy grid. Thus, the memory requirements are proportional to the number of grid cells, which is roughly the mapping area divided by the area of a grid cell. Since cell locations need not be saved because the mapping between cell index and location is known, memory requirements are reduced. On a Lenovo T540p laptop with an Intel Core i7-4900MQ processor (quad-core, 2.8GHz per core) and 16GB of RAM, the mean time for updating the occupancy grid is $0.0115$ seconds, and the mean time to determine an exploration strategy is $0.6820$ seconds. Time requirements for mapping and exploration are easily modified by changing grid cell resolution or the number of exploration pose candidates, respectively. In short, there is a tradeoff between computational speed and accuracy of mapping or exploration.

The autonomous exploration is governed by a policy to maximize expected entropy decreases, so entropy and entropy change with time (Figure \ref{fig:ExperimentH}) are important metrics. In this example, the a priori occupancy probabilities of all cells are $0.1$, so the entropies could temporarily rise for likely occupied cells. The total map entropy begins at roughly $2613$ and decreases by roughly half to $1338$. Many grid cells within the map limits are occluded by walls, preventing the blocked cells from occupancy probability changes. In some cases, the robot is able to update the cell occupancy probabilities; in other cases, cells fall inside regions occluded from all reachable space, and the robot cannot update these cells. Among those grid cells visible from collision-free locations, the vast majority of cells are updated in less than $3$ min while following a very slow ($5$ cm/sec) trajectory.

\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{JIRS_fig11.pdf}
	\caption{Entropy Change During 2D Experiment}
	\medskip
	\small
	The map entropy $H$ averaged over all $90,601$ grid cells is reduced during experimentation. The quick changes $\Delta H$ per cell correspond to map updates when the robot captures new territory.
	\label{fig:ExperimentH}
\end{figure}

In conclusion, this example shows how occupancy grid mapping and autonomous exploration are applied to a ground vehicle in a 2D environment such as a floor of a house or building. Dijkstra's search is applied about the occupancy grid, and a smooth trajectory is easily tracked with a simple controller that tracks a polynomial least squares trajectory. These results show the efficacy of exact occupancy grid mapping and autonomous exploration. Next, we extend these experiments to 3D.


\section{Geometric Control}
\label{sec:GeometricControl}

The next two experiments involve a quadrotor unmanned aerial vehicle (UAV) mapping and exploring 3D environments. A flight controller that accounts for the nonlinear quadrotor attitude dynamics is required to track the trajectories provided by autonomous exploration. We choose geometric control because of its stability properties and simple form.

Geometric control is designed for rigid-body attitude dynamics, and is developed directly on $\SO$ without local parameterizations such as Euler angles or quaternions. Therefore, this approach avoids singularities, ambiguities, and linearization errors. It has been developed for fully-actuated vehicles~\cite{KauCalLeeLee14} and for under-actuated quadrotors~\cite{LeeLeoMcc10,LeeLeo_4457,LeeLeo,GooDaeLee13}. This approach is further extended to aerial load transportation~\cite{GooLee14,GooLee16} and attitude estimation~\cite{WuKauLee15}.

In the context of quadrotor control, the main idea is that a translational position-integral-derivative (PID) control produces a desired force in 3D, and the robot selects a desired attitude $R_d$ and angular velocity $\Omega_d$ to align its propellers with this force and to correct its first body-fixed axis to a desired direction. The attitude and angular velocity errors from the current robot attitude $R$ and angular velocity $\Omega$ are 
\begin{align}
e_R&=(R_d^\T R-R^\T R_d)^{\mathsf{V}},
\\
e_\Omega&=\Omega-R^\T R_d\Omega_d,
\end{align}
respectively, where the \emph{vee} map is defined in~\cite{LeeLeo}. The quadrotor is capable of producing a force $f\in\Re$ and moment $M\in\Re^3$,
\begin{align}
\label{eqn:geometricf}
f&=(k_xe_x+k_ve_v+mge_3-m\ddot{x}_d)\cdot Re_3,
\\
\label{eqn:geometricM}
M&=-k_Re_R-k_\Omega e_\Omega+\Omega\times J\Omega-J(\hat{\Omega}R^\T R_d\Omega_d-R^\T R_d\dot\Omega_d),
\end{align}
where $e_x$ and $e_v$ are position and velocity errors, respectively, $k_x$, $k_v$, $k_R$, $k_\Omega$ are positive gain constants, $\ddot{x}_d$ is the desired translational acceleration, $\dot{\Omega}_d$ is the desired rotational acceleration, $m$ is the quadrotor mass, $J$ is the quadrotor moment of inertia, $e_3=[0, 0, 1]^\T$, and the \emph{hat} map is defined in~\cite{LeeLeo}. Since \refeqn{geometricf} and \refeqn{geometricM} require only trivial calculations, they are easily computed with onboard modules.





\section{Aerial Exploration of a Large Space}
\label{sec:QuadrotorNRL}


In this section, we present results from a quadrotor as it autonomously explores a nearly vertically-uniform room. A robots builds a 3D map of its environment, which is projected onto a 2D map for exploration following the process described in Section~\ref{sec:MapProjections}. The software structure and sensor parameters are identical to those presented in Section~\ref{sec:Compare2MapProjections}.

\subsection{Exploration Environment}

The U.S. Naval Research Laboratory (NRL) has a large experimental space for testing known as the Laboratory for Autonomous Systems Research (LASR). Here, the experimental setup contains walls and objects to resemble a building floor plan. The perimeter walls are constructed with tan cardboard, which were suspended from $1$ m tall stanchions, and an internal wall is built with gray metal sheeting mounted to $80/20$ supports~\cite{url_8020}. Additionally, a hard flooring reflects some florescent lighting above. The objects inside the experimental environment include various trash cans, desks, and chairs in addition to a few miscellaneous objects. The map size is selected to slightly exceed the volume of the experiment such that the x-direction (positive east) spans $0.5$ m to $10.3$ m, the y-direction (positive north) spans $-8.3$ m to $3.5$ m, and the z-direction (positive vertically up) spans $-0.15$ m to $1.5$ m. Two images of the experimental environment are displayed in Figure \ref{fig:exp3DEnvironment}.

\begin{figure}[!t]
\centering
    	\begin{subfigure}[t]{0.95\columnwidth}
           	\centering
          	\includegraphics[width=\textwidth]{experiment_north.jpg}
        		\caption{North Region}
    	\end{subfigure}
    	\begin{subfigure}[t]{0.95\columnwidth}
	\vspace*{0.03\columnwidth}
           	\centering
          	\includegraphics[width=\textwidth]{experiment_south.jpg}
        		\caption{South Region}
    	\end{subfigure}
	\caption{3D Experimental Environment for Level Flight}
	\medskip
	\small
	The 3D experimental environment is designed to resemble a large room within an office building. The walls are built from from cardboard suspended from stanchions and gray sheeting with $80/20$ supports, and miscellaneous objects are placed around the room as well.
	\label{fig:exp3DEnvironment}
\end{figure}


\subsection{Hardware Structure}

Several components are used for localization, sensing, and actuation. A Vicon motion capture system provides the transformation between a reference frame fixed to the world and the moving vehicle. This transformation, along with fixed transformations between the robot body and the sensors, provides sufficient information for the transformation from the world to each sensor frame. The sensor readings from the Xtion and Hokuyo are transmitted from an NVidia Jetson TX2 onboard the vehicle (Figure \ref{fig:QuadrotorHardware}). The sensor readings are received via WIFI on a host computer with an Intel Core i7-6800K CPU (12$\times$3.40GHz), which runs the mapping and exploration nodes. The host computer returns the exploration trajectories to the Jetson TX2 over WIFI. Then, the Jetson executes the geometric flight controller onboard. With this configuration, large processing tasks are avoided onboard the robot.
		
\begin{figure}[!t]
\centering
    	\begin{subfigure}[t]{0.44\columnwidth}
           	\centering
          	\includegraphics[width=\textwidth]{quad_top.png}
        		\caption{Quadrotor Top}
    	\end{subfigure}
	\hspace*{0.05\columnwidth}
    	\begin{subfigure}[t]{0.44\columnwidth}
           	\centering
          	\includegraphics[width=\textwidth]{quad_bottom.png}
        		\caption{Quadrotor Bottom}
    	\end{subfigure}
	\caption{Quadrotor Platform for 3D Mapping and Exploration}
	\medskip
	\small
	A Jetson TX2 (quadrotor top) streams depth measurements via WIFI from an Asus Xtion and Hokuyo LIDAR (quadrotor bottom). The host computer builds a 3D probabilistic occupancy grid map from these measurements. For autonomous exploration, the host computer designs exploration trajectories, which an onboard flight controller can track in real-time. This platform is used for both level flight (Section \ref{sec:QuadrotorNRL}) and non-level flight (Section \ref{sec:QuadrotorSEH}).
	\label{fig:QuadrotorHardware}
\end{figure}

\subsection{Experimental Results}
                
The quadrotor takes off, completes a rotation to understand its immediate surroundings, then explores the space over $2$ minutes and $47$ seconds, shown in Figure \ref{fig:expFilming}. Figure \ref{fig:exp3DMap} shows the probabilistic 3D map generated in real-time at four selected times, and the corresponding 2D projected maps (x-direction upward) are illustrated in Figure \ref{fig:exp2DMap}, where arrows represent pose candidates and a trajectory is shown to the optimal pose. Figure \ref{fig:expH} shows the total map entropy decreasing over the experiment duration. A video of the experiment can be found at \href{https://www.youtube.com/watch?v=I_1rXV2XRqk}{\WriteBlue{https://www.youtube.com/watch?v=I\_1rXV2XRqk}}.


\begin{figure}[!t]
\centering{
    	\begin{subfigure}[t]{0.4\columnwidth}
           	\centering
          	\includegraphics[trim={25cm 20cm 4cm 0cm}, clip, width=\textwidth]{NRL_quadrotor_4panes_0sec.jpg}
        		\caption{$0$ sec}
    	\end{subfigure}
	\hspace*{0.04\columnwidth}
    	\begin{subfigure}[t]{0.4\columnwidth}
           	\centering
          	\includegraphics[trim={25cm 20cm 4cm 0cm}, clip, width=\textwidth]{NRL_quadrotor_4panes_20sec.jpg}
        		\caption{$20$ sec}
    	\end{subfigure}
}
\centering{
    	\begin{subfigure}[t]{0.4\columnwidth}
           	\centering
          	\includegraphics[trim={25cm 20cm 4cm 0cm}, clip, width=\textwidth]{NRL_quadrotor_4panes_40sec.jpg}
        		\caption{$40$ sec}
    	\end{subfigure}
	\hspace*{0.04\columnwidth}
    	\begin{subfigure}[t]{0.4\columnwidth}
           	\centering
          	\includegraphics[trim={25cm 20cm 4cm 0cm}, clip, width=\textwidth]{NRL_quadrotor_4panes_60sec.jpg}
        		\caption{$60$ sec}
    	\end{subfigure}
}
\centering{
    	\begin{subfigure}[t]{0.4\columnwidth}
           	\centering
          	\includegraphics[trim={25cm 20cm 4cm 0cm}, clip, width=\textwidth]{NRL_quadrotor_4panes_80sec.jpg}
        		\caption{$80$ sec}
    	\end{subfigure}
	\hspace*{0.04\columnwidth}
    	\begin{subfigure}[t]{0.4\columnwidth}
           	\centering
          	\includegraphics[trim={25cm 20cm 4cm 0cm}, clip, width=\textwidth]{NRL_quadrotor_4panes_100sec.jpg}
        		\caption{$100$ sec}
    	\end{subfigure}
}
\centering{
    	\begin{subfigure}[t]{0.4\columnwidth}
           	\centering
          	\includegraphics[trim={25cm 20cm 4cm 0cm}, clip, width=\textwidth]{NRL_quadrotor_4panes_120sec.jpg}
        		\caption{$120$ sec}
    	\end{subfigure}
	\hspace*{0.04\columnwidth}
    	\begin{subfigure}[t]{0.4\columnwidth}
           	\centering
          	\includegraphics[trim={25cm 20cm 4cm 0cm}, clip, width=\textwidth]{NRL_quadrotor_4panes_140sec.jpg}
        		\caption{$140$ sec}
    	\end{subfigure}
}
\centering{
    	\begin{subfigure}[t]{0.4\columnwidth}
           	\centering
          	\includegraphics[trim={25cm 20cm 4cm 0cm}, clip, width=\textwidth]{NRL_quadrotor_4panes_160sec.jpg}
        		\caption{$160$ sec}
    	\end{subfigure}
	\hspace*{0.04\columnwidth}
    	\begin{subfigure}[t]{0.4\columnwidth}
           	\centering
          	\includegraphics[trim={25cm 20cm 4cm 0cm}, clip, width=\textwidth]{NRL_quadrotor_4panes_167sec.jpg}
        		\caption{$167$ sec (end)}
    	\end{subfigure}
}
	\caption{Level Flight Experiment}
	\medskip
	\small
	During the first minute, the robot slowly takes off and does an initial rotation, then explores the space in the remaining time.
	\label{fig:expFilming}
\end{figure}


\begin{figure}[!t]
\centering{
    	\begin{subfigure}[t]{0.4\columnwidth}
           	\centering
          	\includegraphics[trim={25cm 1cm 4cm 19cm}, clip, width=\textwidth]{NRL_quadrotor_4panes_0sec.jpg}
        		\caption{$0$ sec}
    	\end{subfigure}
	\hspace*{0.04\columnwidth}
    	\begin{subfigure}[t]{0.4\columnwidth}
           	\centering
          	\includegraphics[trim={25cm 1cm 4cm 19cm}, clip, width=\textwidth]{NRL_quadrotor_4panes_20sec.jpg}
        		\caption{$20$ sec}
    	\end{subfigure}
}
\centering{
    	\begin{subfigure}[t]{0.4\columnwidth}
           	\centering
          	\includegraphics[trim={25cm 1cm 4cm 19cm}, clip, width=\textwidth]{NRL_quadrotor_4panes_40sec.jpg}
        		\caption{$40$ sec}
    	\end{subfigure}
	\hspace*{0.04\columnwidth}
    	\begin{subfigure}[t]{0.4\columnwidth}
           	\centering
          	\includegraphics[trim={25cm 1cm 4cm 19cm}, clip, width=\textwidth]{NRL_quadrotor_4panes_60sec.jpg}
        		\caption{$60$ sec}
    	\end{subfigure}
}
\centering{
    	\begin{subfigure}[t]{0.4\columnwidth}
           	\centering
          	\includegraphics[trim={25cm 1cm 4cm 19cm}, clip, width=\textwidth]{NRL_quadrotor_4panes_80sec.jpg}
        		\caption{$80$ sec}
    	\end{subfigure}
	\hspace*{0.04\columnwidth}
    	\begin{subfigure}[t]{0.4\columnwidth}
           	\centering
          	\includegraphics[trim={25cm 1cm 4cm 19cm}, clip, width=\textwidth]{NRL_quadrotor_4panes_100sec.jpg}
        		\caption{$100$ sec}
    	\end{subfigure}
}
\centering{
    	\begin{subfigure}[t]{0.4\columnwidth}
           	\centering
          	\includegraphics[trim={25cm 1cm 4cm 19cm}, clip, width=\textwidth]{NRL_quadrotor_4panes_120sec.jpg}
        		\caption{$120$ sec}
    	\end{subfigure}
	\hspace*{0.04\columnwidth}
    	\begin{subfigure}[t]{0.4\columnwidth}
           	\centering
          	\includegraphics[trim={25cm 1cm 4cm 19cm}, clip, width=\textwidth]{NRL_quadrotor_4panes_140sec.jpg}
        		\caption{$140$ sec}
    	\end{subfigure}
}
\centering{
    	\begin{subfigure}[t]{0.4\columnwidth}
           	\centering
          	\includegraphics[trim={25cm 1cm 4cm 19cm}, clip, width=\textwidth]{NRL_quadrotor_4panes_160sec.jpg}
        		\caption{$160$ sec}
    	\end{subfigure}
	\hspace*{0.04\columnwidth}
    	\begin{subfigure}[t]{0.4\columnwidth}
           	\centering
          	\includegraphics[trim={25cm 1cm 4cm 19cm}, clip, width=\textwidth]{NRL_quadrotor_4panes_167sec.jpg}
        		\caption{$167$ sec (end)}
    	\end{subfigure}
}
	\caption{3D Occupancy Grid Map During Level Flight Experiment}
	\medskip
	\small
	The 3D occupancy grid map is overlaid with point cloud measurements. The map probabilities directly rely on the Xtion and Hokuyo measurements and the sensor stochastic properties.
	\label{fig:exp3DMap}
\end{figure}

% trim={<left> <lower> <right> <upper>}

%\begin{figure}[!t]
%\centering{
%    	\begin{subfigure}[t]{0.4\columnwidth}
%           	\centering
%          	\includegraphics[trim={5cm 0cm 6cm 0cm}, clip, width=\textwidth]{experiment_ogm3D_0min.jpg}
%        		\caption{$0$ min}
%    	\end{subfigure}
%	\hspace*{0.04\columnwidth}
%    	\begin{subfigure}[t]{0.4\columnwidth}
%           	\centering
%          	\includegraphics[trim={5cm 0cm 6cm 0cm}, clip, width=\textwidth]{experiment_ogm3D_1min.jpg}
%        		\caption{$1$ min}
%    	\end{subfigure}
%}
%\centering{
%    	\begin{subfigure}[t]{0.4\columnwidth}
%           	\centering
%          	\includegraphics[trim={5cm 0cm 6cm 0cm}, clip, width=\textwidth]{experiment_ogm3D_2min.jpg}
%        		\caption{$2$ min}
%    	\end{subfigure}
%	\hspace*{0.04\columnwidth}
%    	\begin{subfigure}[t]{0.4\columnwidth}
%           	\centering
%          	\includegraphics[trim={5cm 0cm 6cm 0cm}, clip, width=\textwidth]{experiment_ogm3D_2min47sec.jpg}
%        		\caption{$2$ min $47$ sec (end)}
%    	\end{subfigure}
%}
%	\caption{3D Occupancy Grid Map During Level Flight Experiment}
%	\medskip
%	\small
%	The 3D occupancy grid map is overlaid with point cloud measurements. The map probabilities directly rely on the Xtion and Hokuyo measurements and the sensor stochastic properties.
%	\label{fig:exp3DMap}
%\end{figure}

\begin{figure}[!t]
\centering{
    	\begin{subfigure}[t]{0.3\columnwidth}
           	\centering
          	\includegraphics[trim={0cm 19.8cm 45.5cm 0cm}, clip, width=\textwidth]{NRL_quadrotor_4panes_0sec.jpg}
        		\caption{$0$ sec}
    	\end{subfigure}
	\hspace*{0.02\columnwidth}
    	\begin{subfigure}[t]{0.3\columnwidth}
           	\centering
          	\includegraphics[trim={0cm 19.8cm 45.5cm 0cm}, clip, width=\textwidth]{NRL_quadrotor_4panes_20sec.jpg}
        		\caption{$20$ sec}
    	\end{subfigure}
	\hspace*{0.02\columnwidth}
	\begin{subfigure}[t]{0.3\columnwidth}
           	\centering
          	\includegraphics[trim={0cm 19.8cm 45.5cm 0cm}, clip, width=\textwidth]{NRL_quadrotor_4panes_40sec.jpg}
        		\caption{$40$ sec}
    	\end{subfigure}
}
\\
\centering{
    	\begin{subfigure}[t]{0.3\columnwidth}
           	\centering
          	\includegraphics[trim={0cm 19.8cm 45.5cm 0cm}, clip, width=\textwidth]{NRL_quadrotor_4panes_60sec.jpg}
        		\caption{$60$ sec}
    	\end{subfigure}
	\hspace*{0.02\columnwidth}
    	\begin{subfigure}[t]{0.3\columnwidth}
           	\centering
          	\includegraphics[trim={0cm 19.8cm 45.5cm 0cm}, clip, width=\textwidth]{NRL_quadrotor_4panes_80sec.jpg}
        		\caption{$80$ sec}
    	\end{subfigure}
	\hspace*{0.02\columnwidth}
    	\begin{subfigure}[t]{0.3\columnwidth}
           	\centering
          	\includegraphics[trim={0cm 19.8cm 45.5cm 0cm}, clip, width=\textwidth]{NRL_quadrotor_4panes_100sec.jpg}
        		\caption{$100$ sec}
    	\end{subfigure}
}
\\
\centering{
    	\begin{subfigure}[t]{0.3\columnwidth}
           	\centering
          	\includegraphics[trim={0cm 19.8cm 45.5cm 0cm}, clip, width=\textwidth]{NRL_quadrotor_4panes_120sec.jpg}
        		\caption{$120$ sec}
    	\end{subfigure}
	\hspace*{0.02\columnwidth}
    	\begin{subfigure}[t]{0.3\columnwidth}
           	\centering
          	\includegraphics[trim={0cm 19.8cm 45.5cm 0cm}, clip, width=\textwidth]{NRL_quadrotor_4panes_140sec.jpg}
        		\caption{$140$ sec}
    	\end{subfigure}
	\hspace*{0.02\columnwidth}
    	\begin{subfigure}[t]{0.3\columnwidth}
           	\centering
          	\includegraphics[trim={0cm 19.8cm 45.5cm 0cm}, clip, width=\textwidth]{NRL_quadrotor_4panes_160sec.jpg}
        		\caption{$160$ sec}
    	\end{subfigure}
}
\\
\centering{
    	\begin{subfigure}[t]{0.3\columnwidth}
           	\centering
          	\includegraphics[width=\textwidth]{experiment_2min47sec_2D_labeled.jpeg}
        		\caption{$167$ sec (end)}
    	\end{subfigure}
}
	\caption{2D Projected Occupancy Grid Map During Level Flight Experiment}
	\medskip
	\small
	The 2D projected map is easily produced in real-time. Candidate future poses, shown by arrows, have more blue color for larger objective functions, found with expected information gains and travel distances. Collision-free waypoints (blue line) serve as input to a constrained polynomial least-squares path (light green curve) for the robot controller to follow, where the desired pose (large axes) is tracked by the onboard controller from the current robot pose (small axes).
	\label{fig:exp2DMap}
\end{figure}

\begin{figure}[!t]
	\centering
	\includegraphics[width=0.85\textwidth]{entropy_experiment_aspect3by1.pdf}
	\caption{Entropy History During Level Flight Experiment}
	\medskip
	\small
	Map entropy decreases as the robot explores and captures more of the experimental environment.
	\label{fig:expH}
\end{figure}

The robot is able to produce a 3D occupancy grid in under $3$ minutes without a human providing a trajectory. The 3D map can be easily interpreted by a human, and provides sufficient information in real-time for autonomous exploration. Optimal poses and collision-free exploration trajectories are calculated by the exploration node in under $1$ second. These quick decisions are vital to the autonomous exploration performance since the map is initially uncertain.

The projected map used for both collision and exploration has several benefits. The robot measures walls and objects with the 3D map, and nicely represents these as occupied spaces with the 2D projected map. Additionally, the exploration computation is low, allowing only for brief hover periods (less than $1$ second) between trajectories. However, the robot exploration does not consider those cells far below the robot, particularly those near the ground. As a result, some floor regions are not captured by the 3D occupancy grid even though cells above the floor are well-known. These neglected cells on the floor motivate the full 3D exploration, shown in the next experiment.

%\newpage

%\let\oldsection\section
%\renewcommand\section{\clearpage\oldsection}

\sectionbreak{\clearpage}
\section{Full 3D Exploration with a Quadrotor}
\label{sec:QuadrotorSEH}

In this final experiment, we introduce the quadrotor to a space with complex 3D objects. The robot generates a 3D map of this space, and performs autonomous exploration while considering the expected information gains of sample measurement rays with nonzero vertical components. Unlike prior examples, the robot also moves vertically during exploration and tracks the ground below the vehicle.

\subsection{3D Exploration Setup and Parameters}
The software follows the same structure as in Section \ref{sec:MarsSimulation}, except the Gazebo simulated environment is replaced with experimental equipment. This equipment is identical to the prior experimental example (Section \ref{sec:QuadrotorNRL}), except the experiment takes place at the Flight Dynamics and Control Lab (FDCL) inside the second floor of SEH, and the Hokuyo is no longer in use. This environment is smaller with only $6$ cameras, where the x-direction (positive east) spans $-4.5$ m to $4.5$ m, the y-direction (positive north) spans $-4.0$ m to $4.5$ m, and the z-direction (positive vertically up) spans $-0.25$ m to $2.0$ m, shown in Figure \ref{fig:Full3DExplorationEnvironment}.

\begin{figure}[!t]
\centering
    	\begin{subfigure}[t]{0.44\columnwidth}
           	\centering
          	\includegraphics[width=\textwidth]{explore3D_space_NW.JPG}
        		\caption{Northwest View}
    	\end{subfigure}
	\hspace*{0.05\columnwidth}
    	\begin{subfigure}[t]{0.44\columnwidth}
           	\centering
          	\includegraphics[width=\textwidth]{explore3D_space_NE.JPG}
        		\caption{Northeast View}
    	\end{subfigure}
	\caption{3D Exploration Space}
	\medskip
	\small
	The experimental space for full 3D exploration consists of normal lab objects such as tables and chairs, as well as a mannequin sitting at a desk and his parked bicycle.
	\label{fig:Full3DExplorationEnvironment}
\end{figure}

\subsection{3D Exploration Results}

The experimental results show that the robot successfully explores a 3D space, capturing occupied space above and below the robot. The quadrotor does an initial rotation over $20$ sec, then explores the space with planned motions in all three dimensions, shown in Figure \ref{fig:exp3DVid}. The robot generates a 3D probabilistic map (Figure \ref{fig:exp3DOccMap}) and uses this for entropy prediction and collision-free motion planning with a 3D cost map (Figure \ref{fig:Full3DExplorationCostmaps}). A video of this experiment is available at \href{https://www.youtube.com/watch?v=2Q2_-d8kNu0}{\WriteBlue{https://www.youtube.com/watch?v=2Q2\_-d8kNu0}}.

\begin{figure}[!t]
\centering{
    	\begin{subfigure}[t]{0.3\columnwidth}
           	\centering
          	\includegraphics[width=\textwidth]{explore3D_vid_0min.jpg}
        		\caption{$0$ sec}
    	\end{subfigure}
	\hspace*{0.02\columnwidth}
    	\begin{subfigure}[t]{0.3\columnwidth}
           	\centering
          	\includegraphics[width=\textwidth]{explore3D_vid_0p25min.jpg}
        		\caption{$15$ sec}
    	\end{subfigure}
	\hspace*{0.02\columnwidth}
	\begin{subfigure}[t]{0.3\columnwidth}
           	\centering
          	\includegraphics[width=\textwidth]{explore3D_vid_0p5min.jpg}
        		\caption{$30$ sec}
    	\end{subfigure}
}
\\
\centering{
    	\begin{subfigure}[t]{0.3\columnwidth}
           	\centering
          	\includegraphics[width=\textwidth]{explore3D_vid_1min.jpg}
        		\caption{$1$ min}
    	\end{subfigure}
	\hspace*{0.02\columnwidth}
    	\begin{subfigure}[t]{0.3\columnwidth}
           	\centering
          	\includegraphics[width=\textwidth]{explore3D_vid_1p5min.jpg}
        		\caption{$1.5$ min}
    	\end{subfigure}
	\hspace*{0.02\columnwidth}
	\begin{subfigure}[t]{0.3\columnwidth}
           	\centering
          	\includegraphics[width=\textwidth]{explore3D_vid_2min.jpg}
        		\caption{$2$ min}
    	\end{subfigure}
}
\\
\centering{
    	\begin{subfigure}[t]{0.3\columnwidth}
           	\centering
          	\includegraphics[width=\textwidth]{explore3D_vid_2p5min.jpg}
        		\caption{$2.5$ min}
    	\end{subfigure}
	\hspace*{0.02\columnwidth}
    	\begin{subfigure}[t]{0.3\columnwidth}
           	\centering
          	\includegraphics[width=\textwidth]{explore3D_vid_3min.jpg}
        		\caption{$3$ min}
    	\end{subfigure}
	\hspace*{0.02\columnwidth}
	\begin{subfigure}[t]{0.3\columnwidth}
           	\centering
          	\includegraphics[width=\textwidth]{explore3D_vid_3p5min.jpg}
        		\caption{$3.5$ min}
    	\end{subfigure}
}
\\
\centering{
    	\begin{subfigure}[t]{0.3\columnwidth}
           	\centering
          	\includegraphics[width=\textwidth]{explore3D_vid_4min.jpg}
        		\caption{$4$ min}
    	\end{subfigure}
	\hspace*{0.02\columnwidth}
    	\begin{subfigure}[t]{0.3\columnwidth}
           	\centering
          	\includegraphics[width=\textwidth]{explore3D_vid_4p5min.jpg}
        		\caption{$4.5$ min}
    	\end{subfigure}
	\hspace*{0.02\columnwidth}
	\begin{subfigure}[t]{0.3\columnwidth}
           	\centering
          	\includegraphics[width=\textwidth]{explore3D_vid_4p75min.jpg}
        		\caption{$4.75$ min (end)}
    	\end{subfigure}
}
	\caption{Full 3D Autonomous Exploration Experiment}
	\medskip
	\small
	The robot completes a full yaw rotation over $20$ seconds, then autonomously explores the space. At $4$ min (j), the robot flies over the mannequin to explore the southeast corner of the room.
	\label{fig:exp3DVid}
\end{figure}



\begin{figure}[!t]
\centering{
    	\begin{subfigure}[t]{0.3\columnwidth}
           	\centering
          	\includegraphics[width=\textwidth]{explore3D_0min.jpg}
        		\caption{$0$ sec}
    	\end{subfigure}
	\hspace*{0.02\columnwidth}
    	\begin{subfigure}[t]{0.3\columnwidth}
           	\centering
          	\includegraphics[width=\textwidth]{explore3D_0p25min.jpg}
        		\caption{$15$ sec}
    	\end{subfigure}
	\hspace*{0.02\columnwidth}
	\begin{subfigure}[t]{0.3\columnwidth}
           	\centering
          	\includegraphics[width=\textwidth]{explore3D_0p5min.jpg}
        		\caption{$30$ sec}
    	\end{subfigure}
}
\\
\centering{
    	\begin{subfigure}[t]{0.3\columnwidth}
           	\centering
          	\includegraphics[width=\textwidth]{explore3D_1min.jpg}
        		\caption{$1$ min}
    	\end{subfigure}
	\hspace*{0.02\columnwidth}
    	\begin{subfigure}[t]{0.3\columnwidth}
           	\centering
          	\includegraphics[width=\textwidth]{explore3D_1p5min.jpg}
        		\caption{$1.5$ min}
    	\end{subfigure}
	\hspace*{0.02\columnwidth}
	\begin{subfigure}[t]{0.3\columnwidth}
           	\centering
          	\includegraphics[width=\textwidth]{explore3D_2min.jpg}
        		\caption{$2$ min}
    	\end{subfigure}
}
\\
\centering{
    	\begin{subfigure}[t]{0.3\columnwidth}
           	\centering
          	\includegraphics[width=\textwidth]{explore3D_2p5min.jpg}
        		\caption{$2.5$ min}
    	\end{subfigure}
	\hspace*{0.02\columnwidth}
    	\begin{subfigure}[t]{0.3\columnwidth}
           	\centering
          	\includegraphics[width=\textwidth]{explore3D_3min.jpg}
        		\caption{$3$ min}
    	\end{subfigure}
	\hspace*{0.02\columnwidth}
	\begin{subfigure}[t]{0.3\columnwidth}
           	\centering
          	\includegraphics[width=\textwidth]{explore3D_3p5min.jpg}
        		\caption{$3.5$ min}
    	\end{subfigure}
}
\\
\centering{
    	\begin{subfigure}[t]{0.3\columnwidth}
           	\centering
          	\includegraphics[width=\textwidth]{explore3D_4min.jpg}
        		\caption{$4$ min}
    	\end{subfigure}
	\hspace*{0.02\columnwidth}
    	\begin{subfigure}[t]{0.3\columnwidth}
           	\centering
          	\includegraphics[width=\textwidth]{explore3D_4p5min.jpg}
        		\caption{$4.5$ min}
    	\end{subfigure}
	\hspace*{0.02\columnwidth}
	\begin{subfigure}[t]{0.3\columnwidth}
           	\centering
          	\includegraphics[width=\textwidth]{explore3D_4p75min.jpg}
        		\caption{$4.75$ min (end)}
    	\end{subfigure}
}
	\caption{3D Occupancy Grid Map During Full 3D Autonomous Exploration}
	\medskip
	\small
	The robot (large axes) is controlled to track the desired pose (small axes), which follows a collision-free trajectory using Dijkstra's search toward candidates (top $75\%$ shown with red arrows, with greater opacity for larger expected information gains).
	\label{fig:exp3DOccMap}
\end{figure}


\begin{figure}[!t]
\centering
    	\begin{subfigure}[t]{0.8\columnwidth}
           	\centering
          	\includegraphics[width=\textwidth]{connected_costmap.png}
        		\caption{Connected Cost Map Cross-Section}
    	\end{subfigure}
    	\begin{subfigure}[t]{0.8\columnwidth}
           	\centering
          	\includegraphics[width=\textwidth]{disconnected_costmap.png}
        		\caption{Disconnected Cost Map Cross-Section}
    	\end{subfigure}
	\caption{2D Cross-Sections of 3D Cost Maps}
	\medskip
	\small
	Two examples of the 3D cost maps from Dijkstra's search show connected and disconnected regions at the exploration height, where more blue regions have smaller distance costs, red regions have larger distance costs, and pink regions are unreachable due to collision risk. In (a), the robot does not have to change altitude to reach any of the collision-free poses at its current altitude. However in (b), the robot can only reach collision-free poses in the disconnected section by changing altitude and following the 3D cost map.
	\label{fig:Full3DExplorationCostmaps}
\end{figure}


The proposed 3D occupancy grid mapping and autonomous exploration approaches complement each other well. The floor and objects are well-captured, as this detail-oriented entropy-based exploration scheme tends to generate completely mapped regions before moving to new terrain, shown with Case 1 of the Mars exploration numerical example (Section \ref{sec:MarsSimulation}). As a result, the entropy decreases fairly steadily over the experiment, shown in Figure \ref{fig:Explore3D_H}.

A key improvement to prior experiments is the ability to consider the occupancy properties in full 3D. This allows the robot to understand that the mannequin occupies some space that the robot must avoid. However, the robot also recognizes that the space above the mannequin is free, and it chooses a trajectory above the mannequin to reach new terrain. At other times, the robot observes spaces from lower vantage points, which can be advantageous with a short sensor range and the forward direction of the Asus Xtion relative to the robot. These added capabilities are attributed to the full 3D mapping and exploration approach.

\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{H_explore3D_test1.pdf}
	\caption{Entropy Change During Full 3D Experiment}
	\medskip
	\small
	The map entropy $H$ consistently decreases during the full 3D exploration experiment. Toward the end of the experiment, most cells that can be captured by the Asus Xtion have been updated, slowing down the rate of entropy decrease.
	\label{fig:Explore3D_H}
\end{figure}

The controller is also highly important for following autonomous exploration trajectories accurately. The geometric controller tracks the desired trajectory to avoid collsions with obstacles in the room. The plot of 3D position is shown in Figure \ref{fig:Explore3D_x3D} and the desired commands and measured outputs of the controlled system are shown in Figures \ref{fig:Explore3D_x_xd}, \ref{fig:Explore3D_v_vd}, \ref{fig:Explore3D_R_Rd}, and \ref{fig:Explore3D_W_Wd}, and the total propeller force $f$ from \refeqn{geometricf} and moment $M$ from \refeqn{geometricM} are shown in Figure \ref{fig:Explore3D_fM}. An interesting aspect is that at $1.31$ min, the robot temporarily obtains a poor estimate of the robot, but since this is short-lived, the robot quickly recovers and continues exploring. In summary, geometric control successfully tracks the desired commands from autonomous exploration through uncertain environments.

\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{explore3D_x3D.pdf}
	\caption{Robot Position over Full 3D Experiment}
	\medskip
	\small
	The history of robot position is shown in 3D with respect to a north-east-down world-fixed frame, such that there are all negative $x_3$ values, making the trajectory appear upside-down. The low locations (high relative to the ground) on the left of the figure ($x_1\approx-1$ m, $x_2\approx0.5$ m, and $x_3\approx-1.8$ m) correspond to flying over the mannequin.
	\label{fig:Explore3D_x3D}
\end{figure}


\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{explore3D_x_xd.pdf}
	\caption{Geometric Control Position Tracking over Full 3D Experiment}
	\medskip
	\small
	The position of the robot (blue solid) tracks the desired position (red dashed) over the experiment.
	\label{fig:Explore3D_x_xd}
\end{figure}

\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{explore3D_v_vd.pdf}
	\caption{Geometric Control Velocity Tracking over Full 3D Experiment}
	\medskip
	\small
	The velocity of the robot (blue solid) tracks the desired velocity (red dashed) over the experiment.
	\label{fig:Explore3D_v_vd}
\end{figure}

\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{explore3D_R_Rd.pdf}
	\caption{Geometric Control Attitude Tracking over Full 3D Experiment}
	\medskip
	\small
	The attitude of the robot (blue solid), shown with the rotation matrix between the north-east-down frame and the body-fixed frame, tracks the desired attitude (red dashed) over the experiment. The bottom-right plot shows the third body-fixed axis is mostly-aligned with the third world-fixed axis, which corresponds to maintaining level flight.
	\label{fig:Explore3D_R_Rd}
\end{figure}

\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{explore3D_W_Wd.pdf}
	\caption{Geometric Control Angular Velocity Tracking over Full 3D Experiment}
	\medskip
	\small
	The angular velocity of the robot (blue solid) tracks the desired angular velocity (red dashed) over the experiment. The overall larger $\Omega_3$ corresponds to yaw rotation to change the direction of the Xtion depth sensor.
	\label{fig:Explore3D_W_Wd}
\end{figure}


\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{explore3D_fM.pdf}
	\caption{Geometric Control Force and Moments over Full 3D Experiment}
	\medskip
	\small
	The forces and moments provided by the geometric control are shown over the experiment. The values of $f$ remain largely-steady to cancel gravitational forces. Changes in $M_1$ and $M_2$ correspond to roll and pitch movements to move around horizontally. Changes in $M_3$ correspond to correcting yaw for the Xtion direction.
	\label{fig:Explore3D_fM}
\end{figure}

